/*
 * Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License").
 * You may not use this file except in compliance with the License.
 * A copy of the License is located at
 *
 *  http://aws.amazon.com/apache2.0
 *
 * or in the "license" file accompanying this file. This file is distributed
 * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing
 * permissions and limitations under the License.
 */

import java.util.ArrayList;
import java.util.List;
import java.util.regex.Pattern;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.StreamingContext;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import scala.Tuple2;

import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
import com.amazonaws.services.kinesis.AmazonKinesisClient;
import com.amazonaws.services.kinesis.model.DescribeStreamRequest;
import com.amazonaws.services.kinesis.model.DescribeStreamResult;
import com.amazonaws.services.kinesis.model.GetRecordsRequest;
import com.amazonaws.services.kinesis.model.GetRecordsResult;
import com.amazonaws.services.kinesis.model.GetShardIteratorRequest;
import com.amazonaws.services.kinesis.model.GetShardIteratorResult;
import com.amazonaws.services.kinesis.model.Record;
import com.amazonaws.services.kinesis.model.Shard;
import com.google.common.collect.Lists;

public class KinesisWorker{

    private static AmazonKinesisClient kinesisClient;
    private static final Log logger = LogFactory.getLog(KinesisClient.class);

    private static final Pattern SPACE = Pattern.compile(" ");

    private static void init() throws Exception {
        /*
         * This credentials provider implementation loads your AWS credentials
         * from a properties file at the root of your classpath.
         */
    	
        kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain());
    }

    public static void main(String[] args) throws Exception {
        init();

        final String myStreamName = "sparkStream";

        DescribeStreamRequest describeStreamRequest = new DescribeStreamRequest();
        describeStreamRequest.setStreamName( myStreamName );
        List<Shard> shards = new ArrayList<>();
        String lastShardId = null;
        DescribeStreamResult describeStreamResult = null;
        do {
            describeStreamRequest.setExclusiveStartShardId( lastShardId );
            describeStreamResult = kinesisClient.describeStream( describeStreamRequest );
            shards.addAll( describeStreamResult.getStreamDescription().getShards() );
            if (shards.size() > 0) {
                lastShardId = shards.get(shards.size() - 1).getShardId();
            }
        } while ( describeStreamResult.getStreamDescription().getHasMoreShards() );
            
        String shardIterator;
        GetShardIteratorRequest getShardIteratorRequest = new GetShardIteratorRequest();
        getShardIteratorRequest.setStreamName(myStreamName);
        getShardIteratorRequest.setShardId(lastShardId);
        getShardIteratorRequest.setShardIteratorType("TRIM_HORIZON");

        GetShardIteratorResult getShardIteratorResult = kinesisClient.getShardIterator(getShardIteratorRequest);
        shardIterator = getShardIteratorResult.getShardIterator();

        //Continuously read data records from shard.
        List<Record> records;

        while (true) {           
			//Create new GetRecordsRequest with existing shardIterator. 
			//Set maximum records to return to 1000.
			GetRecordsRequest getRecordsRequest = new GetRecordsRequest();
			getRecordsRequest.setShardIterator(shardIterator);
			getRecordsRequest.setLimit(1000); 
			
			GetRecordsResult result = kinesisClient.getRecords(getRecordsRequest);
			  
			//Put result into record list. Result may be empty.
			records = result.getRecords();
			
		    // Create the context with a 1 second batch size
		    JavaStreamingContext streamingContext = new JavaStreamingContext(
		    		"local[1]", 
		    		"KinesisWorker",
		            new Duration(1000),
		            null,
		            //System.getenv("SPARK_HOME"),	            
		            //JavaStreamingContext.jarOfClass(KinesisWorker.class),
		            null,
		            null
		            );

		    // Create a NetworkInputDStream on target ip:port and count the
		    // words in input stream of \n delimited text (eg. generated by 'nc')
		    JavaDStream<String> lines = streamingContext.socketTextStream(args[1], Integer.parseInt(args[2]));
		    JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
		      @Override
		      public Iterable<String> call(String x) {
		        return Lists.newArrayList(SPACE.split(x));
		      }
		    });
		    JavaPairDStream<String, Integer> wordCounts = words.map(
		      new PairFunction<String, String, Integer>() {
		        @Override
		        public Tuple2<String, Integer> call(String s) {
		        	return new Tuple2<String, Integer>(s, 1);
		        }
		      }).reduceByKey(new Function2<Integer, Integer, Integer>() {
			        @Override
			        public Integer call(Integer i1, Integer i2) {
			          return i1 + i2;
			        }
		      });

	    	wordCounts.print();
	    	streamingContext.start();
	    	streamingContext.awaitTermination();

        	System.out.println("found more records: " + records);
			  
			try {
			    Thread.sleep(1000);
			} 
			catch (InterruptedException exception) {
			    throw new RuntimeException(exception);
			}
			  
			shardIterator = result.getNextShardIterator();
        }
    }
}